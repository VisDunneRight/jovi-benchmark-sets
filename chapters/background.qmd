# Motivation and Background

This work stems from the challenges we encountered in finding datasets tailored to test graph layout algorithms. When developing a graph layout algorithm which handles specific features (such as layers, or clusters), it is essential to have a benchmark dataset that reflects these features. While conducting our own evaluations, we found it difficult to find datasets that would incorporate features that we needed. This submission is part of an ongoing effort to keep a curated list of datasets. 

Our work focuses on providing a graph benchmark collection that categorizes datasets by how they organize their graphs and emphasizes their features. We aim to facilitate researchers' choice of benchmarks to reflect real use cases or allow comparisons to other algorithms in their respective fields. 
Although a number of graph repositories exist, their target and objectives are not always aligned with the needs of graph drawing researchers.
While the scope of this work revolves around compiling graphs and networks used in the graph drawing literature, we highlight that other, adjacent fields have also created similar **repositories tailored to different needs**. 
For example, the <a href="#Network_Repository" class="dataset-link">Network Repository</a> consists of a comprehensive collection of datasets that contain many attributes and are used for benchmarking in machine learning, data mining, and many other network applications [@nr].
In biology, the <a href="#Biological_Pathways_KEGG" class="dataset-link">KEGG Encyclopedia of Genes and Genomes</a> contains network information relevant to biological pathways [@kegg].
Some general-purpose collections used in network science are also relevant to our discussion. 
Among the most famous ones, the <a href="#SuiteSparse_Matrix_Collection" class="dataset-link">SuiteSparse Matrix Collection</a>, the <a href="#SNAP" class="dataset-link">Stanford Network Analysis Project (SNAP)</a>, and the <a href="#Pajek" class="dataset-link">Pajek</a> collection stand out since they propose large compilations of datasets that often come from diverse sources.
The [Open Graph Benchmark](https://ogb.stanford.edu/) collection from @hu2020 is also worth mentioning, but it is made specifically for training machine learning models, which is not the focus of our work.
Much simpler examples of similar collections can be found in curated lists of links on GitHub, often referred to as *awesome* lists, where a short comment usually accompanies every entry (for instance, [here](https://github.com/gzcsudo/Awesome-Hypergraph-Network) and [here](https://github.com/domargan/awesome-dynamic-graphs)). The purpose and scope of such repositories is provided with more depth in @sec-established-network-repositories. 

Such lists can serve as great tools to find particular case studies, but they do not serve the same purpose as a **uniform collection** like <a href="#Rome-Lib" class="dataset-link">Rome-Lib</a>: a collection of graphs with similar features that can be used to test an algorithm on thousands of graphs with increasing nodes. Rome-Lib is hosted on the main website of the Graph Drawing website, as proof of its usefulness as a benchmark dataset, together with the AT&T graphs and the random DAGs. Another example is the Graph Partitioning Archive, also known as the <a href="#Walshaw" class="dataset-link">Walshaw</a> Collection, which compiles relevant graphs and partitioning algorithms from disparate sources in the relevant literature [@walshaw2000]. The uniformity of such collections allows scientists to easily run thousand of tests on similar graphs, allowing to test the scalability of an algorithm varying density, number of nodes and number of edges --- as opposed to the previously mentioned collections, where the focus is on the diversity of the graphs. See @sec-uniform-benchmark-datasets for more information on this topic.

We care particularly about the **reproducibility** of past and future research. A dataset that has been used in an evaluation and is now unaccessible greatly hinders the reproducibility of the evaluation, and in the worst case it makes it impossible to reproduce, and, as such, much less meaningful. 
Losing a dataset to link rot is an unfortunately common problem in the digital age, as URLs change, websites go down, and data is lost. One example of this is the [Open Graph Archive](https://www.open-graph-archive.org/) from @bachmaier2012, which was a project to create a graph database that categorizes, analyses, and visualizes graphs uploaded from the community, a laudable effort now rendered unfortunately inaccessible. At the time of writing, the popular repository [Konect](http://konect.cc/) is also unavailable and only accessible through the webarchive, although all the links to it in previous papers are now broken. Several other instances are discussed in @sec-lost.

We tried to mitigate the problem of lost datasets by still documenting what we could find about them.
For every dataset that we found that is now lost or inaccessible, we documented every detail we could find about it in literature, including descriptions and pictures of the rendered graphs, so that we can conserve a hint of what the dataset contained. We also went through a process of reaching out to authors and looking into internal storages of research groups (those that were available to us) to find datasets that were not available online. In a couple of cases, this led to successfull outcomes: see <a href="#Storylines_Movie_Plots" class="dataset-link">Storylines (Movie Plots)</a>.

We store our collection on the [Open Science Framework (OSF)](https://osf.io/), which is the currently recommended solution in the VIS community for long-term archival of research data. As per [OSF's backup and preservation policy](https://help.osf.io/article/547-account-and-security-faq-s#Backup), storage and open access is guaranteed for the next 50 years. 

In this context, it is also worth mentioning that in recent years there have been several initiatives aimed at encouraging care for replicability in research.
The [Graphics Replicability Stamp](https://www.replicabilitystamp.org/) is one of these, meant to be an endorsement of the replicability of the results presented in a paper, which ensures the replicability of the results of a paper through an additional review process. Another similar intiative are the [ACM badges](https://www.acm.org/publications/policies/artifact-review-and-badging-current), or the [SIGMOD availability and reproducibility initiative](https://reproducibility.sigmod.org/), which goes one step further and publishes full reports commenting on how reproducible a paper is. 

::: {.callout-note appearance="simple" icon=false}
Because the intention behind this initiative is an ongoing effort to maintain an useful repository of graph datasets, we encourage anyone who may want to correct, integrate or replace information to [reach out to us](#nte-post-abstract). Authors are also welcome to submit a pull request to our [GitHub repository](https://github.com/VisDunneRight/jovi-benchmark-sets).
:::

This work proposes an overarching taxonomy of datasets and collections based on their structure while also providing a higher emphasis on the features and usage within the literature of our field. 
Our collection is offered as a complement to the previously mentioned collections, as we intend to aid researchers in finding graphs in the context of layout algorithms and network visualization, with an accent on encouraging efforts towards replicability. 
