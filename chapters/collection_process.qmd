# Collection process

The information we collected is a by-product of a larger systematic review we conducted related to graph layout algorithms, which included 196 papers The following figure shows the original data collection process (from [@dibartolomeo2024evaluating]):

![The sources used for the collection of papers. We started from the last 7 years of graph drawing, then integrated with the results from IEEE and Eurographics search, and then we also included all the most cited papers in Graph Drawing. The final result was 196 papers containing layout algorithm evaluations.](images/sources3(1).svg){fig.alt="The sources used for the collection of papers. We started from the last 7 years of graph drawing, then integrated with the results from IEEE and Eurographics search, and then we also included all the most cited papers in Graph Drawing. The final result was 196 papers containing layout algorithm evaluations."} 

The core of our data collection was the last seven years of Graph Drawing proceedings (264 papers in total), filtering out papers without computational evaluations.
We further expanded our graph layout algorithm papers collection by searching IEEE Xplore and Wiley digital libraries to include papers from TVCG and CGF. Then, we checked all the citations in the papers we collected from Graph Drawing, and added to our collection all the papers that were cited more than 4 times in the last 6 years of graph drawing --- to make sure we included algorithm papers that were important, but not published at GD, on IEEE Xplore or on the Wiley digital library.
For each paper, we collected which features were handled by the graph layout algorithm presented, and what dataset was used in the evaluation. When collecting features, we always prioritized the authors' own wording and description of the features. The tagging of the papers was done by two people at the same time, over two different passes for sanity-checking purposes. 
Following this process, we tried to track down the datasets used in computational evaluations: (1) we first looked for official or linked supplemental material, (2) we next Googled the dataset or paper name, (3) finally, we emailed the authors. 
When in doubt about the artifact replication policy, we asked the owners or authors by email.
In cases where it was explicitly mentioned that approval should be received before redistribution, we did not redistribute the datasets. However, if we received approval or did not receive an answer and found no explicit policy preventing redistirbution, we collected and stored the dataset to preserve it for future researchers. 
If any dataset owner or author discovers their own work in our collection and would like it removed, we kindly request that they contact us (see @sec-authorship below), and we will promptly remove it. Furthermore, we want to emphasize that we do not assert any ownership rights over the datasets listed.

The chart below shows the distribution of papers across different venues:

```{ojs}
//| echo: false
{ 
  let d = []
  let allbenchmarkdatasets = [...new Set(literature.map(l => l["Conference"].split(",").map(d => d.trim())).flat().map(a => {
    if (a == "EuroVis") return "Computer Graphics Forum"
    else return a;
  }))].filter(a => a != "")

  for (let dataset of allbenchmarkdatasets){
    let literature_entries_with_this_dataset = literature.filter(l => l["Conference"].includes(dataset))

    if (literature_entries_with_this_dataset.length < 2) continue;

    d.push({"dataset": dataset, "count": literature_entries_with_this_dataset.length})
  }
  
  return Plot.plot({
    
    color: {
      range: ["steelblue"]
    },
    y: {},
    marks: [
      Plot.barX(d, {x: "count", y: "dataset", fill: "dataset",inset: 2, sort: {y: "x", reverse: true}}),
      Plot.axisY({label: null, lineWidth: 12, marginLeft: 150, marginTop: 20}),
      Plot.text(['Venues where the papers where published'], {frameAnchor: "Top",dy: -10})
    ]
  })
}
```

The following one, instead, shows the distribution of collected papers' publication date:

```{ojs}
//| echo: false
{ 
  let d = []
  let year_range = [1980, 2023]
  for (let year = year_range[0]; year < year_range[1]; year++){
    let literature_entries_with_this_year = literature.filter(l => parseInt(l["Year"]) == year);

    d.push({"year": year, "count": literature_entries_with_this_year.length})
  }

  const tickPositions = d.map((d, i) => i).filter(i => i % 5 === 0);
  
  return Plot.plot({
    color: {
      range: ["steelblue"]
    },
    y: {
      tickValues: tickPositions
    },
    height: 500,
    marks: [
      Plot.barX(d, {x: "count", y: "year", fill: "steelblue", inset: 2}),
      Plot.axisY({label: null, lineWidth: 12, marginLeft: 150, marginTop: 20, tickFormat: d3.format("d"), tickValues: tickPositions}),
      Plot.text(['Year of publication of the papers included in the survey'], {frameAnchor: "Top",dy: -10})
    ]
  })
}
```

After collecting the datasets, we looked more in-depth into their contents, running analysis on a number of statistics associated with the graphs contained in them. 

We finally ended up collecting 46 datasets, listed below:

```{ojs}
//| echo: false
//| output: false
{ 
  let allbenchmarkdatasets = [...new Set(literature.map(l => l["Dataset tag relations"].split(",").map(a => a.split("(")[0].trim())).flat())].filter(a => a != "")
  let collection_types = benchmark_datasets.map(a => a["Type of Collection"]);

  // console.log(allbenchmarkdatasets.filter(b => !b.includes("Custom").length))
  // console.log(allbenchmarkdatasets.filter(b => b.includes("Custom").length))

  let s = ""
  s += "| Dataset Name | Papers using this dataset | \n"
  s += "| --- | --- | \n"
  
  for (let dataset of allbenchmarkdatasets){
    let literature_entries_with_this_dataset = literature.filter(l => l["Dataset tag relations"].includes(dataset))
    let d_entry = benchmark_datasets.find(b => b["Name"].includes(dataset))
    // if (d_entry == undefined) console.log("not found: ", dataset);
    // else console.log(d_entry)

    if (literature_entries_with_this_dataset.length < 2) continue;

    s += `| ${dataset} | ` 
    for (let entry of literature_entries_with_this_dataset){
      s += "[@" + entry["Citation name"] + "] "
    }
    s += "| \n"
  }
  // console.log(s)
}
```

<!-- | Dataset Name | Papers using this dataset | 
| --- | --- | 
| Assorted Collaboration Network | [@saket2016] [@Ogawa2010] [@minghim2006] [@gove2019] [@mizuno2019] [@Qu2022] [@Henry2007] [@burch2011] [@Bach2014] [@Zhao2021] [@Gansner2013] [@Lin2010] [@diBartolomeo2022] [@Dang2016] [@Grabowicz2014] [@Simonetto2018] [@Angori2019] [@DiGiacomo2020] [@arleo2022] | 
| Code Dependency Graphs | [@devkota2018] [@Tanahashi2015] [@Holten2006] [@burch2011] | 
| Transportation Networks | [@zheng2019] [@wang2018] [@muelder2008b] [@fisheye2019] [@bast2020] [@preiner2020] [@gove2019] [@zeng2019] [@Huang2016] [@Nollenburg2011] [@Meulemans2013] [@greilich2009] [@brandes1998] [@muelder2008] [@batik2022] | 
| SuiteSparse Matrix Collection | [@zheng2019] [@DRGraph21] [@zhong2023] [@Gansner2013] [@kruiger2017] [@Eades2018] [@Ortmann2017] [@Hong2019] [@Brsig2020] [@miller2023] [@meidiana2023] | 
| SNAP | [@zhong2023] [@gove2019] [@Jacomy2014] [@Arleo2019] [@Meidiana2019] [@meidiana2023] | 
| Social Networks | [@wang2018] [@fisheye2019] [@gove2019] [@hoffswell2018] [@Qu2022] [@Zhao2021] [@nocaj2015] [@Raj2018] [@Simonetto2018] [@Hachul2006] [@Ni2018] [@Purchase2020] | 
| Enron | [@vandenElzen2013] [@Tanahashi2015] | 
| California | [@muelder2008b] [@muelder2008] | 
| Airlines | [@wang2020] [@Wallinger2022] [@WeiweiCui2008] [@vonLandesberger2016] [@Ersoy2011] [@Holten2009] [@Wu2018] [@Perri2020] [@wallinger2023] | 
| Migrations | [@wang2020] [@Wallinger2022] [@WeiweiCui2008] [@vonLandesberger2016] [@Ersoy2011] [@Holten2009] [@Wu2018] [@Perri2020] [@wallinger2023] | 
| and Air Traffic | [@wang2020] [@Wallinger2022] [@WeiweiCui2008] [@vonLandesberger2016] [@Ersoy2011] [@Holten2009] [@Wu2018] [@Perri2020] [@wallinger2023] | 
| Rome-Lib | [@Buchheim2008] [@chimani2010] [@chimani2006] [@diBartolomeo2022b] [@Clancy2019] [@DiBattista1997b] [@chimani2016b] [@Chimani2008] [@Bekos2018] [@Demel2018] [@Heinsohn2018] [@Kindermann2018] [@chimani2016] [@Chimani2012] [@Gansner2006] [@Gutwenger2004] [@klammler2018] [@Niedermann2020] [@Giovannangeli2021] [@Chimani2021] | 
| North DAGs | [@chimani2010] [@chimani2016b] [@DIBATTISTA2000] [@Demel2018] [@Binucci2016] [@Regg2016] [@chimani2016] [@klammler2018] [@Chimani2021] [@binucci2022] | 
| Stanford GraphBase | [@gove2019] [@barth2002] [@junger1997] [@kruiger2017] [@Raj2018] [@Gronemann2016b] | 
| Walshaw | [@gove2019] [@Frishman2008] [@Gansner2005b] [@KOREN20051867] [@hu2005efficient] [@Eades2018] [@Hachul2006] [@Hachul2005] [@Hong2019] [@civril2006] [@brandes2006] [@hacul2005] [@koren2002] [@frishman2007] [@walshaw2001] [@meidiana2020] | 
| Storylines | [@diBartolomeo2022b] [@Tanahashi2015] [@ShixiaLiu2013] [@Tanahashi2012] [@Bartolomeo2021] [@vanDijk2018] [@Gronemann2016b] | 
| Militarized Interstate Disputes | [@ShixiaLiu2013] [@Tanahashi2012] | 
| Neural Network | [@Liu2017] [@Wongsuphasawat2018] | 
| Blogposts | [@Frishman2008] [@Dang2016] [@Grabowicz2014] [@Purchase2020] [@arleo2022] | 
| Tweets | [@Frishman2008] [@Dang2016] [@Grabowicz2014] [@Purchase2020] [@arleo2022] | 
| and Forums | [@Frishman2008] [@Dang2016] [@Grabowicz2014] [@Purchase2020] [@arleo2022] | 
| AT&T | [@barth2002] [@DIBATTISTA2000] [@Jabrayilov2016] [@Chimani2012] [@Hachul2006] [@Hachul2005] [@Mallach2019] [@Jnger2018] [@hacul2005] [@welch2017] | 
| KnownCR | [@Clancy2019] [@Chimani2012] [@Chimani2021] | 
| World Bank Trade Data | [@nocaj2015] [@noack2004] | 
| Pajek | [@noack2004] [@Meidiana2019] | 
| Matrix Market | [@KOREN20051867] [@Gansner2005] [@klammler2018] [@brandes2006] [@meidiana2020] | 
| Padia Stories | [@padia2019] [@padia2018] | 
| Protein Interactions | [@Dang2016] [@muelder2008] | 
| GION | [@Eades2018] [@Marner2014] | 
| Network Repository | [@Eades2018] [@Arleo2019] | 
| Graphviz Examples | [@Nachmanson2017] [@Gange2011] [@giovannangeli2023] | 
| Scotch Graph Collection | [@koren2002] [@harel2001] |  -->

The purpose of our collection effort is to facilitate comparisons and replication of results, and to provide a resource for researchers to find datasets that are used in the literature. We also provide links to the specific papers that use each dataset, so that they can be used as examples and sources of information. 