---
title: "Benchmark Datasets for Graph Layout Algorithms"
number-sections: true
self-contained: true
author: 
  - name: Sara Di Bartolomeo
    orcid: 0000-0001-9517-3526
    email: sara.di-bartolomeo@uni-konstanz.de
    affiliations:
      - name: University of Konstanz
        city: Konstanz
        country: Germany
  - name: Tarik Crnovrsanin
    orcid: 0000-0002-4397-5532
    email:
    affiliations:
      - name: Northeastern University
        city: Boston
        state: MA
        country: USA
  - name: Connor Wilson
    orcid: 0000-0002-6936-4078
    email:
    affiliations:
      - name: Northeastern University
        city: Boston
        state: MA
        country: USA
  - name: Eduardo Puerta
    orcid: 0000-0003-4664-4365
    email:
    affiliations:
      - name: Northeastern University
        city: Boston
        state: MA
        country: USA
  - name: Cody Dunne
    orcid: 0000-0002-1609-9776
    email: c.dunne@northeastern.edu
    affiliations:
      - name: Northeastern University
        city: Boston
        state: MA
        country: USA
    
bibliography: bibliography.bib

---
<!-- 
This template is intended for the [experimental track](https://www.journalovi.org/submit.html#experimental) at JoVI.

This template describes how to set up the [Quarto](https://quarto.org/) environment for writing a JoVI article, provides instructions and templates for content that should be included in every JoVI article (such as the structured abstract and authorship metadata), and gives brief examples of some useful functionality for writing papers (such as how to include figures and citations).

If you have not setup [Quarto](https://quarto.org/) or the JoVI template yet, skip to the [Setup](#setup) section below. -->

::: {.callout-note appearance="simple" icon="false" collapse="false"}
## Abstract {.unnumbered}

<!-- We highly recommend all JoVI articles use a structured abstract. Structured abstracts provide a succinct overview of an article using a common set of sections. Which sections are used depends on the type of research and its goals. For example, articles with empirical methods should have a data collection section, while articles that do not analyze data to support a conclusion do not need such a section.

Your paper may fall into multiple types of work. We provide some example sections you may want to include in your structured abstract below, depending on the type of work. You can use the example sections to fill in the subsections that apply to your paper, then delete these first two paragraphs and the subsections that do not apply to your paper. Please add, merge, rename, or re-order sections if you feel that would improve clarity. We have some (partial) examples of structured abstracts [here](https://www.journalovi.org/structured-abstract-examples.html). -->

### Introduction

Computational evaluations are crucial for standardized and objective evaluation of graph layout algorithms. Standard benchmark datasets facilitate comparison with prior work, and reliable access to datasets is fundamental for replicability. However, there is no comprehensive repository of benchmark datasets for Graph Drawing, and many datasets have been lost.

### Data collection

We collected 196 papers from Graph Drawing, IEEE venues, and Eurographics venues that include computational evaluations of graph layout algorithms. We searched for the datasets used.

### Data analysis

We archived datasets we found and re-created ones that were lost but had sufficient replication instructions. We classified graphs by their features and statistics. We also found text and images from papers using those graphs.

### Implementation

We implemented graph creation, data analysis, and website code.

### Materials 	

We provide a [Graph Benchmark Datasets](https://visdunneright.github.io/gd_benchmark_sets/) website; a [repository](https://github.com/visdunneright/gd_benchmark_sets/) with code for the website and metadata on the benchmark datasets; a [long-term archive](https://osf.io/j7ucv/) of this repository and the benchmark datasets.

### Conclusion

We provide a resource for the Graph Drawing and visualization communities to use to find datasets for computational evaluations of graph layout algorithms. Our organization by features and statistics supports rapid identification of relevant graphs. We have re-created and archived graphs used in research for their long-term preservation.

<!-- ## Motivation + Background

## General discussion on datasets and methodology

## The website

-\> have a section on long-term preservation and replicability -\> methodology and what is within the scope -\> possible contribution: how we classified the datasets ---\> describe the iterative methodology of the survey collection, how many authors made passes through it, how we progressively collected the features -\> what is an assorted collection etc etc -\> how to use the website -\> extracting everything and putting it in a consistent format (multiple consistent formats) -\> code for the datasets was released -\> the historical archeology type-stuff we had to do to get the datasets (e.g. emails for permission to host, any clarifications) -\> collection of all the descriptions by hand -\> concerns about lost collections, maintainability of this one -\> movie plots dataset

###### Theory/model

*For theories or models*: A description of the theory or model, its key organizing principle, or new insights that might be gained from it

###### Data Collection or Source

*For empirical research, reanalysis, or meta-analysis*: An overview of experiment and/or data collection procedures. Mention if the collection was prespecified (via preregistration) or exploratory. For reanalyses or meta-analyses, citations of the original sources are sufficient. This section does not apply to articles that only use datasets for demonstrations.

###### Data Analysis

*For empirical research, reanalysis, or meta-analysis*: An overview of analysis approaches used as evidence for conclusions. This section does not apply to articles that only use datasets for demonstrations.

###### Analysis Results

*For empirical research, reanalysis, or meta-analysis*: A summary of the analysis findings

###### Implementation

*For systems or techniques*: How was the application or system implemented?

###### Demonstration

*For systems or techniques*: A description of any demonstrations or functionality that exemplify the utility of the system

###### Conclusion

An interpretation of results, lessons learned, etc. in the context of the research question and its implications. *Avoid overgeneralizing*, and *avoid broad behavioral claims without strong evidence.*

###### Materials

Link to repositories containing raw data, open source code or (pre-)registration number/links. -->
:::

::: {.callout-note appearance="simple" icon="false" collapse="true"}
## Materials, Authorship, License, Conflicts

###### Research materials

<!-- See [the corresponding section in the author guide](https://www.journalovi.org/author-guide.html#research-material-statements). -->

###### Authorship

<!-- See [the corresponding section in the author guide](https://www.journalovi.org/author-guide.html#authorship). -->

###### License

This work is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/).

###### Conflicts of interest

See [the corresponding section in the author guide](https://www.journalovi.org/author-guide.html#conflicts-of-interest).
:::

<!-- # Introduction -->

# Introduction

Benchmarking is a crucial aspect of computer science, as it allows researchers, developers, and engineers to compare the performance of various systems, algorithms, or hardware. A benchmark is a standardized test or set of tests used to measure and compare the performance of hardware, software, or systems under specific conditions. Benchmarking aims to provide objective and consistent metrics that allow for fair comparisons and informed decision-making. Benchmarks are widely used in various fields, including computer hardware evaluation, software optimization, and system performance analysis.
In all these fields, benchmarking provides a standardized and objective way to compare and assess the performance of different systems, algorithms, or software implementations. It aids in making informed decisions about which solution best suits a specific use case or requirement.

The same is true for graph drawing, particularly for studying the performance and results of graph layout algorithms [@dibartolomeo2024evaluating]. 
Benchmark datasets can provide a standardized set of graphs with known properties and characteristics.
These graphs can vary in size, density, connectivity, and structure.
Researchers can objectively compare their performance or the quality of their results by applying various graph layout algorithms to the same benchmark dataset. 

In our own work, we have faced challenges in determining which benchmark datasets to use for evaluating the layout algorithms we developed. This led us to build a collection of benchmark datasets used in previous graph layout algorithm papers and a [Graph Benchmark Datasets](https://visdunneright.github.io/gd_benchmark_sets/) website for perusing the collection. We collected 196 papers from Graph Drawing, IEEE venues, and Eurographics venues that include computational evaluations of graph layout algorithms. We then searched for the datasets used for the benchmarks. We collected the data we could find and had permission to archive and re-created datasets that were lost but had sufficient replication instructions. We classified graphs by their features and statistics. We also found text and images from papers using those graphs. 

This paper aims to present this graph drawing benchmark sets resource to the Graph Drawing and visualization communities so that other authors may benefit from our archiving and organization efforts. We hope this resource will encourage the discoverability of these datasets and the ease of running benchmarks for graph layout algorithms. Moreover, as reliable access to datasets is fundamental for replicability, we aim to preserve these datasets in perpetuity. Beyond collecting available datasets and re-creating lost ones, we archived all our materials [on OSF](https://osf.io/j7ucv/) for long-term availability. This included saving each graph in multiple common file formats to avoid translation issues for individual authors. We believe our work will lead to more reproducible and replicable Graph Drawing research by providing a long-term and open archive of the data we use in our computational evaluations.

Specifically, we contribute:

- A collection of benchmark datasets used for graph layout algorithm evaluations, including:
  - Re-creating lost datasets based on paper descriptions and a list of unavailable data,
  - A classification of these datasets by graph features and statistics to aid in identifying appropriate evaluation candidates,
  - Exemplar text and images from papers that use these datasets,
  - A website for perusing this collection, and
  - A long-term archive of our metadata and the collection to aid in reproducibility and replicability of evaluations.

Please see @sec-research-statements below for our supplemental materials, including links to the website, code and data, OSF archive, and Notion database.
<!-- 
```{python}
import pandas as pd
benchmark_data = pd.read_csv("data/Benchmark_datasets.csv")
literature_data = pd.read_csv("data/Literature.csv")
ojs_define(literature_pre = literature_data)
``` 
-->

```{ojs}
//| echo: false
literature_pre = await FileAttachment("data/Literature.csv").csv()
benchmark_datasets_pre = await FileAttachment("data/Benchmark_datasets.csv").csv()
literature = literature_pre.filter(d => !d.Name.includes("EXCLUDE") && !d.Name == "")
benchmark_datasets = benchmark_datasets_pre.filter(l => l.Name != "")
paper_sources_pre = await FileAttachment("data/Paper Sources.csv").csv()
paper_source = paper_sources_pre.filter(l => l.Name != "")
```

{{< include chapters/collection_process.qmd >}} 

{{< include chapters/datasets_in_use.qmd >}} 

## Details of the datasets

For each of the datasets collected as part of our process, we conducted a brief analysis of their contents. Where possible, the analysis includes information about the number of nodes per graph, the source of the dataset, which papers have used the dataset and what graph features they took into account. The following section defaults to illustrating the contents of Rome-Lib, but the reader can easily change the dataset in focus by modifying the value of the dropdown menu below:

<style>
  .container {
    --bs-badge-padding-x: 0.65em;
    --bs-badge-padding-y: 0.35em;
    --bs-badge-font-size: 0.75em;
    --bs-badge-font-weight: 700;
    --bs-badge-color: #fff;
    --bs-badge-border-radius: 50rem;
    display:flex;

  }
  .btn {
    --bs-btn-padding-y: .15rem;
    --bs-btn-padding-x: 0.5rem;
    --bs-btn-font-size: .7rem;
    margin-left: 0.2rem;
    margin-right: 0.2rem;
    padding: var(--bs-btn-padding-y) var(--bs-btn-padding-x);
    padding-top: 0.05rem;
    font-size: var(--bs-btn-font-size);
    border-radius: .25rem;
    padding-bottom: 0.05rem;
  }
  
  .btn-secondary {
    background-color:#6c757d;
    color:#fff !important;
  }
  .btn-secondary:hover{
    background-color:#565e64;
    
  }
  .a {
    color:rgb(13, 110, 253);
  }
  .a:hover{
    color:#0a58ca;
  }

  .card {
  }
  .card-body {
  }
  .row {
    display:flex;
  }
  .col-8 {
    flex: 0 0 auto;
    width: 66.66666667%;
  }
  .col-4 {
    flex: 0 0 auto;
    width: 33.33333333%;
  }
  .small-text{
    font-size:small;
  } 
  .rounded-pill {
    border-radius: var(--bs-badge-border-radius) !important;
  }
  .badge {
    display: inline-block;
    padding: var(--bs-badge-padding-y) var(--bs-badge-padding-x);
    font-size: var(--bs-badge-font-size);
    font-weight: var(--bs-badge-font-weight);
    line-height: 1;
    color: var(--bs-badge-color);
    text-align: center;
    white-space: nowrap;
    vertical-align: baseline;
  }

  .pill-tag {
    background-color: #ddd;
    border: none;
    color: white;
    padding: 0px 8px;
    text-align: center;
    font-weight: bold;
    display: inline-block;
    margin: 0px -5px;
    border-radius: 20px;
    transform: scale(0.8);
  }

  .pill-tag-generic {
    background-color: rgb(217, 115, 13);
  }

  .pill-tag-large {
    background-color: rgb(224, 62, 62);
  }

  .pill-tag-weighted-edges {
    background-color: rgb(223, 171, 1)
  }
</style>

<!-- code to generate the collapsible dataset info things -->
```{ojs}
//| echo: false
//| output: false
{
  let s = ""
  for (let dataset_to_be_visualized of benchmark_datasets.map(d => d.Name)){
    s += `::: {.callout-note title="` + dataset_to_be_visualized + `" collapse=true appearance="minimal"}` + "\n\n"
    s += "```{ojs}\n"
    s += `//| echo: false\n
make_sparkline("` + name_cleanup(dataset_to_be_visualized) + `")\n`
    s += "```\n\n"
    s += `<div id="named-list-` + name_cleanup(dataset_to_be_visualized) + `" data-bs-spy="scroll"  data-bs-target="db-nav-list" data-bs-offset="20" tabindex="0"></div>\n\n`
    s += "::: \n\n\n"
  }
  // console.log(s);
}
```

{{< include chapters/dataset_detail_collapsibles.qmd >}} 

```{ojs}
//| echo: false
//| output: false
viewof dataset_to_be_visualized = Inputs.select(benchmark_datasets.map(b => b.Name), {value: "Rome-Lib", label: "Dataset to visualize"})
```

```{ojs}
//| echo: false
function make_sparkline(bname){
  let b = benchmark_datasets.find(l => name_cleanup(l.Name) == bname)["sparkline data"].replaceAll("'", '"')
  try {
    b = JSON.parse(b)
    return Plot.plot({
    color: {
      range: ["steelblue"]
    },
    y: {nice:true},
    x: {nice:true},
    height: 400,
    marks: [
      Plot.line(b.num_nodes.map((d, i) => [i*5, d]), {marginTop: 30}),
      Plot.text(['Distribution of graph sizes by number of nodes in ' + bname], {frameAnchor: "Top",dy: -20})
    ]
  })
  } catch { return "No sparkline information to show" }
}
```

<script>
document.addEventListener('DOMContentLoaded', (event) => {
    document.querySelectorAll('a.dataset-link').forEach(item => {
        item.addEventListener('click', function (e) {
            // jumping is done by default with these links, we just need to get the collapsible to open up
            let str = this.getAttribute('href')
            // console.log('Link to ' + str + ' was clicked!');
            // Link to #enron was clicked!

            // Prepare the identifier
            // Remove the first character and get the second character (now the first)
            const ident = str.slice(1);
            // Capitalize the new first character and add the rest of the string
            // let ident = withoutFirst.charAt(0).toUpperCase() + withoutFirst.slice(1);
            
            // TODO: handle idents where there are spaces and commas etc, just use a map from the link to the title 
            // name of the dataset, like: { link_name: title_name
            //      "enron": "Enron", 
            //      "chess_games": "Chess Games", 
            //      "airlines_migrations_airtraffic": "Airlines, Migrations, and Air Traffic"
            //   }

            // find all the divs classed callout-titled and close them
            document.querySelectorAll('.callout-titled').forEach(item => {
                item.children[0].setAttribute('aria-expanded', 'false');
                item.children[1].classList.remove('show');
            });

            // find a div where the result of the function name_cleanup on its title attribute is equal to ident
            let foundDiv = [...document.querySelectorAll(".callout-titled")].find(d => 
              typeof d.getAttribute('title') === 'string' 
              && (
                name_cleanup(d.getAttribute('title')) == ident || d.getAttribute("data-collapse-id") == ident
                )
              );

            if (foundDiv) {
                console.log('Found div:', foundDiv);
            
                // Accessing the children of foundDiv
                const children = foundDiv.children;
                if (children.length >= 2) {
                    // Assume the first child is the header and the second is the content container
                    const header = children[0];
                    const content = children[1];

                    // Set aria-expanded to true on the header, this rotates the arrow icon on the right
                    header.setAttribute('aria-expanded', 'true');

                    // Add the 'show' class to the content container, this enables the content to be shown
                    content.classList.add('show');
                } else {
                  console.error('Expected at least two children in the div');
                }
            } else {
              console.error('Div not found');
            }
        });
    });
});

</script>







<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

```{ojs}
//| echo: false
//| output: false
{
  for (let benchmark_dataset of benchmark_datasets.map(d => d.Name)){
    let bname = name_cleanup(benchmark_dataset)

    let named_list_dom = d3.select("#named-list-" + bname);
    named_list_dom.selectAll("*").remove()
    let entry =  benchmark_datasets.find(l => name_cleanup(l.Name) == bname);
    const paper_data = literature
    const origin_paper_data = paper_source
    add_entry_row(named_list_dom, entry, paper_data, origin_paper_data, notionFolders)

  }
}
```

```{ojs}
//| echo: false
async function add_entry_row(named_list_dom, entry, paper_data, origin_paper_data, notionFolders){

    if (entry["graph features handled"] != "" && entry["graph features handled"] != undefined){
        let tags = entry["graph features handled"].split(",")
        for (let j = 0; j < tags.length; j++){
            if (tags[j].charAt(0) == ' '){
                tags[j] = tags[j].substring(1).toLowerCase()
            }
            if (!graph_feature_tags.includes(tags[j])){
                graph_feature_tags.push(tags[j])
            }
        }
    }

    for (let element of paper_data){
        let features = (element["Graph feature"]).split(",")
        for (let feature of features){
            feature = feature.trim().toLowerCase()
            if (!graph_feature_tags.includes(feature)){
                graph_feature_tags.push(feature)
            }
        }
    }
        
        let infocontainer = named_list_dom.append("div")
            .attr("class", "container")
            .attr("id", "inforow_" + name_cleanup(entry["Name"]))
            .style("padding", "2%")

        let inforow = infocontainer.append("div")
            .append("div")
            .attr("class", "card card-body")
            .append("div")
            .attr("class", "col small-text")

        add_simple_field(entry, "Originally found at", inforow)
        inforow.append("br")
        add_download_field(entry, inforow);
        inforow.append("br")
        add_simple_field(entry, "Note", inforow)
        // inforow.append("br")
        // add_simple_field(entry, "Number of Files", inforow)
        inforow.append("br")
        add_simple_field(entry, "Size", inforow)
        inforow.append("br")
        add_simple_field(entry, "Origin Notes", inforow)
        inforow.append("br")
        inforow.append("div").html("<b>Origin paper: </b>")
        for (let e of entry["Origin paper plaintext"].split(",")){
            e = e.trim();
            if (e == "") continue;
            let origindoi = (paper_data.find(a => a.Name == e) || {}).DOI
            let cited_paper = origin_paper_data.find(o => o.Name == e)
            let cite_index = origin_paper_data.indexOf(cited_paper)
            // if (origindoi != undefined && origindoi != ""){
                inforow.append("div").attr("class", "row").html('<div class="col-4">[link]</div><div class="col-8">' + e + '</div>')
            // } else inforow.append("div").html(e)
        }

        inforow.append("br")
        inforow.append("div").html("<b>Usage examples: </b>")

        for (let i in entry["Related to Literature - Algorithm (Dataset tag relations) 1"].split(",")){
            let e = entry["Related to Literature - Algorithm (Dataset tag relations) 1"].split(",")[i].split("(")[0].trim()
            let entryrow = inforow.append("div").attr("class", "row")

            if (e != "") {
                let doi = (paper_data.find(a => a.Name == e) || {}).DOI
                let citation_name = (paper_data.find(a => a.Name == e) || {})["Citation name"]
                let link = "";
                // if (doi != undefined && doi != "") link = '<a href="' + doify(doi) + '">[link]</a>'
                if (doi != undefined && doi != "" && citation_name != undefined) {
                  // link += '<span class="citation" data-cites="' + citation_name + '">'
                  // link += '<a href="#ref-' + citation_name + '" role="doc-biblioref" aria-expanded="false">[link]</a>'
                  // link += '</span>'
                  link += "[@" + citation_name + "]"
                } else link = ""

                entryrow.append("div")
                    .attr("class", "col-4")
                    .html(link)

                entryrow.append("div")
                    .attr("class", "col-4")
                    .html(e)

                let tagrow = entryrow.append("div").attr("class", "col-4")

                if (paper_data.find(a => a.Name == e)){
                    let feature_collection = (paper_data.find(a => a.Name == e) || {})["Graph feature"].split(",")
                    
                    for (let graph_feature of feature_collection){
                        graph_feature = graph_feature.trim().toLowerCase()
                        tagrow.append("div")
                            .attr("class", "rounded-pill badge")
                            .style("margin", "2px")
                            .style("background-color", graph_feature_tags_colors[graph_feature_tags.indexOf(graph_feature)%graph_feature_tags_colors.length])
                            .text(graph_feature)
                    }
                }
            }
        }

        inforow.append("br")

        // DRAW REST OF THE PAGE: uncomment when moving over
  
        let otherinfo = ""
        try{
            let otherinfo = await fetch("data/Benchmark datasets 64e0439269f9497799025562a4087ce1/" + entry["Name"].replace(".", "") + " " + entry["Page id"] + ".md")
               
            otherinfo = await otherinfo.text() 
            
            var converter = new showdown.Converter();
            
            otherinfo = getTextAfterBody(otherinfo);
            // let otherinfo2 = otherinfo.replace(/\!\[(.*?)\]\((.*?)\)/g, "![$1](data/Benchmark datasets 64e0439269f9497799025562a4087ce1/$2)");
            // console.log(otherinfo2);

            let string_to_print = ""
            
            for (let line of otherinfo.split("\n")){
                if (line == ">" || line == "> ") continue;
                else if (line[0] == "!") {
                    // get everything after the first (, concat the rest
                    let img = line.split("(")
                    // concat all the entries after the first
                    for (let i = 2; i < img.length; i++){
                        img[1] += "(" + img[i]
                    }
                    // remove the last )
                    img[1] = img[1].substring(0, img[1].length - 1)
                    img = img[1]

                    // inforow.append("div")
                    //     .style("text-align", "center")
                    //     .append("img")
                    //     .attr("src", "data/Benchmark datasets 64e0439269f9497799025562a4087ce1/" + img)
                    //     .attr("width", "80%")

                    string_to_print += "<div style='text-align: center'><img src='data/Benchmark datasets 64e0439269f9497799025562a4087ce1/" + img + "' width='80%'></div>\n"
                // } else if (line.includes("STOP RENDERING")){
                //      break;
                } else string_to_print += line + "\n"
                
            }

            console.log(string_to_print)

        } catch (e){
            console.log(e)
        }

        console.log(entry, inforow.node().innerHTML.replaceAll("<br>", "<br>\n"))
}
```

```{ojs}
//| echo: false
//| output: false
notionFolders = Object()
graph_feature_tags = []
graph_feature_tags_colors = ["#9B9A97", "#64473A", "#D9730D", "#DFAB01", "#0F7B6C", "#0B6E99", "#6940A5", "#AD1A72", "#E03E3E"]
```

<script>
function name_cleanup(name){
  return name.replaceAll("(", "").replaceAll(")", "").replaceAll(" ", "_").replaceAll(".", "").replaceAll("'", "").replaceAll("&", "").replaceAll(" ", "").replaceAll(",", "")
}
</script>

```{ojs}
//| echo: false
function doify(text){
        if (text == "" || text == undefined) return text;
        if (text.includes("https")) return (text);
        else return ("https://doi.org/" + text);
    }
```

```{ojs}
//| echo: false
function add_simple_field(entry, field_name, div, name = ""){
        if (entry[field_name] != "" && entry[field_name] != undefined){
            let title = name === "" ? field_name : name;
            div.append("div").html("<b>" + title + ": </b>" + urlify(entry[field_name]))
        }
    }
```

```{ojs}
//| echo: false
function add_download_field(entry, div){
        let divSection = div.append("div").html("<b>" + "Download formatted data: " + "</b>");
        const list = ['OSF link gexf', 'OSF link gml', 'OSF link graphml', 'OSF link json']

        list.forEach((item)=>{
            if(entry[item] != "" && entry[item] != undefined){
                divSection.append("a")
                // .attr('type', 'button')
                .attr('href', entry[item])
                .attr('class', 'btn btn-secondary btn-sm skinny-button')
                .text(item.split("OSF link").pop().toUpperCase())
            }   
        })
    }
```

```{ojs}
//| echo: false
function urlify(text) {
        var urlRegex = /(([a-z]+:\/\/)?(([a-z0-9\-]+\.)+([a-z]{2}|aero|arpa|biz|com|coop|edu|gov|info|int|jobs|mil|museum|name|nato|net|org|pro|travel|local|internal))(:[0-9]{1,5})?(\/[a-z0-9_\-\.~]+)*(\/([a-z0-9_\-\.]*)(\?[a-z0-9+_\-\.%=&amp;]*)?)?(#[a-zA-Z0-9!$&'()*+.=-_~:@/?]*)?)(\s+|$)/gi;
        return text.replace(urlRegex, function(url) {
            return '<a href="' + url + '">' + url + '</a>';
        })
    }
```

```{ojs}
//| echo: false
//| output: false
showdown = require('https://cdnjs.cloudflare.com/ajax/libs/showdown/2.1.0/showdown.min.js')
```

```{ojs}
//| echo: false
function getTextAfterBody(input) {
        const lines = input.split('\n');
        let emptyLineCount = 0;
        let text = '';

        for (let i = 0; i < lines.length; i++) {
            const line = lines[i].trim();
            
            if (line === '# Body') {
                text = lines.slice(i + 1).join('\n');
                // MathJax.typeset();
            } 
        }
        
        return text;
    }
```

```{ojs}
//| echo: false
//| output: false
dict_of_graph_features_to_colors = {
  let all_graph_features = [... new Set(benchmark_datasets.map(a => a["graph features handled"].split(",").map(a => a.trim())).flat().filter(a => a != ""))]
  let d = {}
  let colorscheme = d3.interpolateRainbow;
  let count = 0;
  for (let f in all_graph_features){
    d[all_graph_features[f]] = d3.color(colorscheme(count++/all_graph_features.length)).darker(0.7)
  }
  return d;
}
```

{{< include chapters/random_generation.qmd >}} 

{{< include chapters/conclusion.qmd >}} 

<!--

## Interesting questions to answer

Has the type of benchmark dataset used changed through the years?

Do papers include supplemental material more frequently compared to previous years?

-> Include a navigable analysis of the contents of the Datasets

-> Include a table with dataset -> paper, split by graph feature

-> Classification of the datasets

## Examples

Let's take a look at the AT&T dataset: -->
<!-- ```{python}
atnt_set = benchmark_data[benchmark_data["Name"] == "AT&T"]

# display the field page id
atnt_set.iloc[0]
``` -->

<!-- ```{python}
atnt_set = benchmark_data[benchmark_data["Name"] == "AT&T"]

# display the field page id
atnt_set.iloc[0]["Related to Literature - Algorithm (1) (Dataset tag relations)"]
``` -->

<!-- ```{python}
atnt_set = benchmark_data[benchmark_data["Name"] == "AT&T"]

# display the field page id
atnt_set.iloc[0]["Graph features in papers"]
``` -->

<!-- ## Setting up and writing a JoVI article {#setup}

Articles on the JoVI experimental track are written in Quarto, which is a simple markdown-based text format. This template outlines some of the features that are most useful for writing academic articles in Quarto; for more visit <https://quarto.org>.

You can find the source for this article in the [journalovi/jovi-template-quarto](https://github.com/journalovi/jovi-template-quarto) repository on Github. To create a new JoVI article, we recommend following these steps:

1.  Install Quarto by following [these instructions](https://quarto.org/docs/getting-started/installation.html).

2.  Create a new git repository to hold your article by [forking our template repository](https://github.com/journalovi/jovi-template-quarto).

You can then edit `index.qmd` in your new repository to write your paper. There are several options for editing and rendering the paper:

1.  You can execute `quarto serve` from the commandline to render the paper to `index.html` and preview it in the browser.

2.  You can edit the paper in RStudio and render it by clicking on the *Render* button:

    ![](images/rstudio-render-button.png){style="width: 382.5px" fig.alt="Click Render in the Rstudio toolbar to render a Quarto document."}

**If you plan to use RStudio,** read more about RStudio and Quarto [here](https://quarto.org/docs/computations/using-rstudio.html). In particular, you should install RStudio \> version 1.5. If you do, you can also enable the visual editor:

![](images/rstudio-visual-editor.png){style="width: 554.5px" fig.alt="Click the Settings button and then Use Visual Editor in the Rstudio toolbar to enable the visual editor."}

## Citations and footnotes

Citations can be inserted using `[@simkin2002read]`, which is rendered as: [@simkin2002read]. You can also format the citation just as the year with `[-@simkin2002read]`, though then we recommending including author names in the sentence; e.g. `Simkin and Roychowdhury [-@simkin2002read] said blah blah blah`: Simkin and Roychowdhury [-@simkin2002read] said blah blah blah. Citations for this paper are kept in `bibliography.bib` in BibTeX format.

You can also insert footnotes.[^1] Note that some traditional uses of footnotes (e.g. to include URLs) are unnecessary in JoVI, as you can (and should) instead simply link directly to the URL you want to refer to with an [inline link](https://www.markdownguide.org/basic-syntax/#links).

[^1]: Like this one!

For more on footnotes and citations see [this Quarto documentation page](https://quarto.org/docs/authoring/footnotes-and-citations.html).

## Figures and images

Images can be included and for accessibility purposes **should always** have alt text. Alt text of visualizations should provide meaningful descriptions of the visualization.

``` markdown
![](images/teaser.svg){fig.alt="Mean expected payoff / optimal
payoff for 10 conditions, showing an increase over time in most
conditions, with dot50 having the highest value of
approximately 97% of optimal."}
```

![](images/teaser.svg){fig.alt="Mean expected payoff / optimal payoff for 10 conditions, showing an increase over time in most conditions, with dot50 having the highest value of approximately 97% of optimal."}

You can also add captions and figure references using a `:::` block combined with an id prefixed with `#fig-`:

``` markdown
::: {#fig-teaser}
![](images/teaser.svg){fig.alt="Mean expected payoff / optimal
payoff for 10 conditions, showing an increase over time in most
conditions, with dot50 having the highest value of
approximately 97% of optimal."}

Mean expected payoff / optimal payoff for 10 conditions.
:::
```

::: {#fig-teaser}
![](images/teaser.svg){fig.alt="Mean expected payoff / optimal payoff for 10 conditions, showing an increase over time in most conditions, with dot50 having the highest value of approximately 97% of optimal."}

Mean expected payoff / optimal payoff for 10 conditions.
:::

Then you can refer to the above figure in text using `@fig-teaser`; for example:

Refer to @fig-teaser. For more on cross-references, see the [Quarto page on cross-references](https://quarto.org/docs/authoring/cross-references.html).

For the highest-quality output, we recommend using SVG figures if possible.

Quarto also contains classes for more complex figure layouts and subfigures, which we recommend using in most cases if you need more complex layouts; see [the Quarto page on figures](https://quarto.org/docs/authoring/figures-and-layout.html). That said, we encourage exploration of what is possible with online article formats, and you should feel free to experiment so long as the result is archivable, accessible, and readable.

## Body text

The basic body text, header formatting, and basic tags such as links, emphasis, etc should be left as-is except in rare cases (for which you should be able to demonstrate the value of your modifications). That said, we do encourage experimentation with what is possible in this format; some possibilities that just scrath the surface include: using color or other formatting to link text content semantically with figures; using inline figures; experimenting with interactive widgets in text and figures. So long as content retains archivability, accessibility, and readability, we welcome experimentation with the format.

## Code

Quarto supports R and python code; for example:

## Equations

Equations can be included using LaTeX syntax:

$$
x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}
$$

For more on equations, see [the corresponding section in the Quarto documentation](). -->

## References {.unnumbered}

::: {#refs}
:::