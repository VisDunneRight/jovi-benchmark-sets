---
title: "Benchmark Datasets for Graph Layout Algorithms"
author: 
  - name: Sara Di Bartolomeo
    orcid: 0000-0001-9517-3526
    email: sara.di-bartolomeo@uni-konstanz.de
    affiliations:
      - name: University of Konstanz
        city: Konstanz
        country: Germany
  - name: Connor Wilson
    orcid: 0000-0002-6936-4078
    email:
    affiliations:
      - name: Northeastern University
        city: Boston
        state: MA
        country: USA
  - name: Eduardo Puerta
    orcid: 0000-0003-4664-4365
    email:
    affiliations:
      - name: Northeastern University
        city: Boston
        state: MA
        country: USA
  - name: Tarik Crnovrsanin
    orcid: 0000-0002-4397-5532
    email:
    affiliations:
      - name: Northeastern University
        city: Boston
        state: MA
        country: USA
  - name: Cody Dunne
    orcid: 0000-0002-1609-9776
    email: c.dunne@northeastern.edu
    affiliations:
      - name: Northeastern University
        city: Boston
        state: MA
        country: USA
    
bibliography: bibliography.bib
# * **Sara Di Bartolomeo** [![orcid_16x16@2.gif](${await FileAttachment("orcid_16x16@2.gif").url()}) https://orcid.org/0000-0001-9517-3526](https://orcid.org/0000-0001-9517-3526): Conceptualization, Data Collection & Categorization, Writing Original Draft + Review & Editing, Visualization, Validation.
# * **Eduardo Puerta** [![orcid_16x16@2.gif](${await FileAttachment("orcid_16x16@2.gif").url()}) https://orcid.org/0000-0003-4664-4365](https://orcid.org/0000-0003-4664-4365): Data Collection & Categorization, Writing Original Draft + Review & Editing, Validation.
# * **Connor Wilson** [![orcid_16x16@2.gif](${await FileAttachment("orcid_16x16@2.gif").url()}) https://orcid.org/0000-0002-6936-4078](https://orcid.org/0000-0002-6936-4078): Data Collection & Categorization, Writing Original Draft + Review & Editing, Validation.
# * **Tarik Crnovrsanin** [![orcid_16x16@2.gif](${await FileAttachment("orcid_16x16@2.gif").url()}) https://orcid.org/0000-0002-4397-5532](https://orcid.org/0000-0002-4397-5532): Data Collection & Categorization, Writing Original Drafr + Review & Editing, Validation.
# * **Cody Dunne** [![orcid_16x16@2.gif](${await FileAttachment("orcid_16x16@2.gif").url()}) https://orcid.org/0000-0002-1609-9776](https://orcid.org/0000-0002-1609-9776): Supervision, Writing Original Draft + Review & Editing, Validation, Funding Acquisition.
---
<!-- 
This template is intended for the [experimental track](https://www.journalovi.org/submit.html#experimental) at JoVI.

This template describes how to set up the [Quarto](https://quarto.org/) environment for writing a JoVI article, provides instructions and templates for content that should be included in every JoVI article (such as the structured abstract and authorship metadata), and gives brief examples of some useful functionality for writing papers (such as how to include figures and citations).

If you have not setup [Quarto](https://quarto.org/) or the JoVI template yet, skip to the [Setup](#setup) section below. -->

::: {.callout-note appearance="simple" icon="false" collapse="false"}
## Abstract {.unnumbered}

<!-- We highly recommend all JoVI articles use a structured abstract. Structured abstracts provide a succinct overview of an article using a common set of sections. Which sections are used depends on the type of research and its goals. For example, articles with empirical methods should have a data collection section, while articles that do not analyze data to support a conclusion do not need such a section.

Your paper may fall into multiple types of work. We provide some example sections you may want to include in your structured abstract below, depending on the type of work. You can use the example sections to fill in the subsections that apply to your paper, then delete these first two paragraphs and the subsections that do not apply to your paper. Please add, merge, rename, or re-order sections if you feel that would improve clarity. We have some (partial) examples of structured abstracts [here](https://www.journalovi.org/structured-abstract-examples.html). -->

### Introduction

Computational evaluations are crucial for standardized and objective evaluation of graph layout algorithms. Standard benchmark datasets facilitate comparison with prior work, and reliable access to datasets is fundamental for replicability. However, there is no comprehensive repository of benchmark datasets for Graph Drawing, and many datasets have been lost.

### Data collection

We collected 196 papers from Graph Drawing, IEEE venues, and Eurographics venues that include computational evaluations of graph layout algorithms. We searched for the datasets used.

### Data analysis

We archived datasets we found and re-created ones that were lost but had sufficient replication instructions. We classified graphs by their features and statistics. We also found text and images from papers using those graphs.

### Implementation

We implemented graph creation, data analysis, and website code.

### Materials 	

We provide a [Graph Benchmark Datasets](https://visdunneright.github.io/gd_benchmark_sets/) website; a [repository](https://github.com/visdunneright/gd_benchmark_sets/) with code for the website and metadata on the benchmark datasets; a [long-term archive](https://osf.io/j7ucv/) of this repository and the benchmark datasets.

### Conclusion

We provide a resource for the Graph Drawing and visualization communities to use to find datasets for computational evaluations of graph layout algorithms. Our organization by features and statistics supports rapid identification of relevant graphs. We have re-created and archived graphs used in research for their long-term preservation.

<!-- ## Motivation + Background

## General discussion on datasets and methodology

## The website

-\> have a section on long-term preservation and replicability -\> methodology and what is within the scope -\> possible contribution: how we classified the datasets ---\> describe the iterative methodology of the survey collection, how many authors made passes through it, how we progressively collected the features -\> what is an assorted collection etc etc -\> how to use the website -\> extracting everything and putting it in a consistent format (multiple consistent formats) -\> code for the datasets was released -\> the historical archeology type-stuff we had to do to get the datasets (e.g. emails for permission to host, any clarifications) -\> collection of all the descriptions by hand -\> concerns about lost collections, maintainability of this one -\> movie plots dataset

###### Theory/model

*For theories or models*: A description of the theory or model, its key organizing principle, or new insights that might be gained from it

###### Data Collection or Source

*For empirical research, reanalysis, or meta-analysis*: An overview of experiment and/or data collection procedures. Mention if the collection was prespecified (via preregistration) or exploratory. For reanalyses or meta-analyses, citations of the original sources are sufficient. This section does not apply to articles that only use datasets for demonstrations.

###### Data Analysis

*For empirical research, reanalysis, or meta-analysis*: An overview of analysis approaches used as evidence for conclusions. This section does not apply to articles that only use datasets for demonstrations.

###### Analysis Results

*For empirical research, reanalysis, or meta-analysis*: A summary of the analysis findings

###### Implementation

*For systems or techniques*: How was the application or system implemented?

###### Demonstration

*For systems or techniques*: A description of any demonstrations or functionality that exemplify the utility of the system

###### Conclusion

An interpretation of results, lessons learned, etc. in the context of the research question and its implications. *Avoid overgeneralizing*, and *avoid broad behavioral claims without strong evidence.*

###### Materials

Link to repositories containing raw data, open source code or (pre-)registration number/links. -->
:::

::: {.callout-note appearance="simple" icon="false" collapse="true"}
## Materials, Authorship, License, Conflicts

###### Research materials

<!-- See [the corresponding section in the author guide](https://www.journalovi.org/author-guide.html#research-material-statements). -->

###### Authorship

<!-- See [the corresponding section in the author guide](https://www.journalovi.org/author-guide.html#authorship). -->

###### License

This work is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/).

###### Conflicts of interest

See [the corresponding section in the author guide](https://www.journalovi.org/author-guide.html#conflicts-of-interest).
:::

## Introduction

<!-- ```{python}
#| output: false
#| echo: false
import matplotlib.pyplot as plt
import numpy as np
from matplotlib.ticker import MaxNLocator
import pandas as pd
import seaborn as sns

benchmark_data = pd.read_csv("data/Benchmark datasets.csv")
literature_data = pd.read_csv("data/Literature.csv")
``` -->

<!-- # Introduction -->

# Introduction

Benchmarking is a crucial aspect of computer science, as it allows researchers, developers, and engineers to compare the performance of various systems, algorithms, or hardware. A benchmark is a standardized test or set of tests used to measure and compare the performance of hardware, software, or systems under specific conditions. Benchmarking aims to provide objective and consistent metrics that allow for fair comparisons and informed decision-making. Benchmarks are widely used in various fields, including computer hardware evaluation, software optimization, and system performance analysis.
In all these fields, benchmarking provides a standardized and objective way to compare and assess the performance of different systems, algorithms, or software implementations. It aids in making informed decisions about which solution best suits a specific use case or requirement.

The same is true for graph drawing, particularly for studying the performance and results of graph layout algorithms **IMPORTANT: TODO cite CGF**. 
Benchmark datasets can provide a standardized set of graphs with known properties and characteristics.
These graphs can vary in size, density, connectivity, and structure.
Researchers can objectively compare their performance or the quality of their results by applying various graph layout algorithms to the same benchmark dataset. 

In our own work, we have faced challenges in determining which benchmark datasets to use for evaluating the layout algorithms we developed. This led us to build a collection of benchmark datasets used in previous graph layout algorithm papers and a [Graph Benchmark Datasets](https://visdunneright.github.io/gd_benchmark_sets/) website for perusing the collection. We collected 196 papers from Graph Drawing, IEEE venues, and Eurographics venues that include computational evaluations of graph layout algorithms. We then searched for the datasets used for the benchmarks. We collected the data we could find and had permission to archive and re-created datasets that were lost but had sufficient replication instructions. We classified graphs by their features and statistics. We also found text and images from papers using those graphs. 

This paper aims to present this graph drawing benchmark sets resource to the Graph Drawing and visualization communities so that other authors may benefit from our archiving and organization efforts. We hope this resource will encourage the discoverability of these datasets and the ease of running benchmarks for graph layout algorithms. Moreover, as reliable access to datasets is fundamental for replicability, we aim to preserve these datasets in perpetuity. Beyond collecting available datasets and re-creating lost ones, we archived all our materials [on OSF](https://osf.io/j7ucv/) for long-term availability. This included saving each graph in multiple common file formats to avoid translation issues for individual authors. We believe our work will lead to more reproducible and replicable Graph Drawing research by providing a long-term and open archive of the data we use in our computational evaluations.

Specifically, we contribute:

- A collection of benchmark datasets used for graph layout algorithm evaluations, including:
  - Re-creating lost datasets based on paper descriptions and a list of unavailable data,
  - A classification of these datasets by graph features and statistics to aid in identifying appropriate evaluation candidates,
  - Exemplar text and images from papers that use these datasets,
  - A website for perusing this collection, and
  - A long-term archive of our metadata and the collection to aid in reproducibility and replicability of evaluations.

Please see [Research Material Statements](#cell-501) **todo: Fix this when the quarto is finished** below for our supplemental materials, including links to the website, code and data, OSF archive, and Notion database.


<!-- {{< include chapters/background.qmd >}}

{{< include chapters/methodology.qmd >}}

## Interesting questions to answer

Has the type of benchmark dataset used changed through the years?

Do papers include supplemental material more frequently compared to previous years?

-> Include a navigable analysis of the contents of the Datasets

-> Include a table with dataset -> paper, split by graph feature

-> Classification of the datasets

{{< include chapters/example_evaluation.qmd >}}

## Examples

Let's take a look at the AT&T dataset: -->
<!-- ```{python}
atnt_set = benchmark_data[benchmark_data["Name"] == "AT&T"]

# display the field page id
atnt_set.iloc[0]
``` -->

<!-- ```{python}
atnt_set = benchmark_data[benchmark_data["Name"] == "AT&T"]

# display the field page id
atnt_set.iloc[0]["Related to Literature - Algorithm (1) (Dataset tag relations)"]
``` -->

<!-- ```{python}
atnt_set = benchmark_data[benchmark_data["Name"] == "AT&T"]

# display the field page id
atnt_set.iloc[0]["Graph features in papers"]
``` -->

<!-- ## Setting up and writing a JoVI article {#setup}

Articles on the JoVI experimental track are written in Quarto, which is a simple markdown-based text format. This template outlines some of the features that are most useful for writing academic articles in Quarto; for more visit <https://quarto.org>.

You can find the source for this article in the [journalovi/jovi-template-quarto](https://github.com/journalovi/jovi-template-quarto) repository on Github. To create a new JoVI article, we recommend following these steps:

1.  Install Quarto by following [these instructions](https://quarto.org/docs/getting-started/installation.html).

2.  Create a new git repository to hold your article by [forking our template repository](https://github.com/journalovi/jovi-template-quarto).

You can then edit `index.qmd` in your new repository to write your paper. There are several options for editing and rendering the paper:

1.  You can execute `quarto serve` from the commandline to render the paper to `index.html` and preview it in the browser.

2.  You can edit the paper in RStudio and render it by clicking on the *Render* button:

    ![](images/rstudio-render-button.png){style="width: 382.5px" fig.alt="Click Render in the Rstudio toolbar to render a Quarto document."}

**If you plan to use RStudio,** read more about RStudio and Quarto [here](https://quarto.org/docs/computations/using-rstudio.html). In particular, you should install RStudio \> version 1.5. If you do, you can also enable the visual editor:

![](images/rstudio-visual-editor.png){style="width: 554.5px" fig.alt="Click the Settings button and then Use Visual Editor in the Rstudio toolbar to enable the visual editor."}

## Citations and footnotes

Citations can be inserted using `[@simkin2002read]`, which is rendered as: [@simkin2002read]. You can also format the citation just as the year with `[-@simkin2002read]`, though then we recommending including author names in the sentence; e.g. `Simkin and Roychowdhury [-@simkin2002read] said blah blah blah`: Simkin and Roychowdhury [-@simkin2002read] said blah blah blah. Citations for this paper are kept in `bibliography.bib` in BibTeX format.

You can also insert footnotes.[^1] Note that some traditional uses of footnotes (e.g. to include URLs) are unnecessary in JoVI, as you can (and should) instead simply link directly to the URL you want to refer to with an [inline link](https://www.markdownguide.org/basic-syntax/#links).

[^1]: Like this one!

For more on footnotes and citations see [this Quarto documentation page](https://quarto.org/docs/authoring/footnotes-and-citations.html).

## Figures and images

Images can be included and for accessibility purposes **should always** have alt text. Alt text of visualizations should provide meaningful descriptions of the visualization.

``` markdown
![](images/teaser.svg){fig.alt="Mean expected payoff / optimal
payoff for 10 conditions, showing an increase over time in most
conditions, with dot50 having the highest value of
approximately 97% of optimal."}
```

![](images/teaser.svg){fig.alt="Mean expected payoff / optimal payoff for 10 conditions, showing an increase over time in most conditions, with dot50 having the highest value of approximately 97% of optimal."}

You can also add captions and figure references using a `:::` block combined with an id prefixed with `#fig-`:

``` markdown
::: {#fig-teaser}
![](images/teaser.svg){fig.alt="Mean expected payoff / optimal
payoff for 10 conditions, showing an increase over time in most
conditions, with dot50 having the highest value of
approximately 97% of optimal."}

Mean expected payoff / optimal payoff for 10 conditions.
:::
```

::: {#fig-teaser}
![](images/teaser.svg){fig.alt="Mean expected payoff / optimal payoff for 10 conditions, showing an increase over time in most conditions, with dot50 having the highest value of approximately 97% of optimal."}

Mean expected payoff / optimal payoff for 10 conditions.
:::

Then you can refer to the above figure in text using `@fig-teaser`; for example:

Refer to @fig-teaser. For more on cross-references, see the [Quarto page on cross-references](https://quarto.org/docs/authoring/cross-references.html).

For the highest-quality output, we recommend using SVG figures if possible.

Quarto also contains classes for more complex figure layouts and subfigures, which we recommend using in most cases if you need more complex layouts; see [the Quarto page on figures](https://quarto.org/docs/authoring/figures-and-layout.html). That said, we encourage exploration of what is possible with online article formats, and you should feel free to experiment so long as the result is archivable, accessible, and readable.

## Body text

The basic body text, header formatting, and basic tags such as links, emphasis, etc should be left as-is except in rare cases (for which you should be able to demonstrate the value of your modifications). That said, we do encourage experimentation with what is possible in this format; some possibilities that just scrath the surface include: using color or other formatting to link text content semantically with figures; using inline figures; experimenting with interactive widgets in text and figures. So long as content retains archivability, accessibility, and readability, we welcome experimentation with the format.

## Code

Quarto supports R and python code; for example:

## Equations

Equations can be included using LaTeX syntax:

$$
x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}
$$

For more on equations, see [the corresponding section in the Quarto documentation](). -->

## References {.unnumbered}

::: {#refs}
:::